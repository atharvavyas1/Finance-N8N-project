{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ca3e17ec",
      "metadata": {
        "id": "ca3e17ec"
      },
      "source": [
        "# Business Logic\n",
        "---\n",
        "This is going to be an experimentation for natural language processing in the context of finance. Using libraries like FinBERT, and LDA, we shall leverage language models to help us inform people about what's going in the world of a specific company. Practices will be followed by extracting the article for a given stock and reusing\n",
        "the methodddology for analysis here.\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb4f0c4",
      "metadata": {
        "id": "cdb4f0c4"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser"
      ],
      "metadata": {
        "id": "EkgDYebP76az"
      },
      "id": "EkgDYebP76az",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import re\n",
        "import torch"
      ],
      "metadata": {
        "id": "xp_ACz0x7B7h"
      },
      "id": "xp_ACz0x7B7h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text extraction"
      ],
      "metadata": {
        "id": "haU5u6H9zvjp"
      },
      "id": "haU5u6H9zvjp"
    },
    {
      "cell_type": "code",
      "source": [
        "class YahooFinanceFullArticleScraper:\n",
        "    \"\"\"\n",
        "    Extracts full article content from Yahoo Finance RSS feeds\n",
        "    Uses RSS for article discovery, then fetches full content from article URLs\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.base_rss_url = \"https://feeds.finance.yahoo.com/rss/2.0/headline?s={}&region=US&lang=en-US\"\n",
        "        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "\n",
        "    def _extract_article_text(self, url):\n",
        "        \"\"\"Extract full article text from a Yahoo Finance article URL\"\"\"\n",
        "        try:\n",
        "            response = requests.get(url, headers=self.headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Find article content - Yahoo Finance uses various selectors\n",
        "            article_content = None\n",
        "\n",
        "            # Try common article content selectors\n",
        "            selectors = [\n",
        "                'article',\n",
        "                '[data-module=\"ArticleBody\"]',\n",
        "                '.caas-body',\n",
        "                '.article-body',\n",
        "                '[class*=\"article\"]',\n",
        "                '[class*=\"content\"]'\n",
        "            ]\n",
        "\n",
        "            for selector in selectors:\n",
        "                article_content = soup.select_one(selector)\n",
        "                if article_content:\n",
        "                    break\n",
        "\n",
        "            if not article_content:\n",
        "                # Fallback: find main content area\n",
        "                article_content = soup.find('main') or soup.find('article')\n",
        "\n",
        "            if article_content:\n",
        "                # Remove script and style elements\n",
        "                for script in article_content([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
        "                    script.decompose()\n",
        "\n",
        "                # Extract text and clean it\n",
        "                text = article_content.get_text(separator=' ', strip=True)\n",
        "                # Clean up multiple whitespaces\n",
        "                text = re.sub(r'\\s+', ' ', text).strip()\n",
        "                return text\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "    def get_full_articles_for_ticker(self, ticker, max_articles=10, verbose=False):\n",
        "        \"\"\"\n",
        "        Get full article content for a ticker symbol\n",
        "\n",
        "        Args:\n",
        "            ticker (str): Stock ticker symbol\n",
        "            max_articles (int): Maximum number of articles to fetch\n",
        "            verbose (bool): Print progress messages\n",
        "\n",
        "        Returns:\n",
        "            list: List of articles with full text content\n",
        "        \"\"\"\n",
        "        articles = []\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Fetching RSS feed for {ticker}...\")\n",
        "\n",
        "        try:\n",
        "            # Get RSS feed\n",
        "            feed_url = self.base_rss_url.format(ticker.upper())\n",
        "            feed = feedparser.parse(feed_url)\n",
        "\n",
        "            if not feed.entries:\n",
        "                if verbose:\n",
        "                    print(f\"  âš  No articles found for {ticker}\")\n",
        "                return articles\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Found {len(feed.entries)} articles in RSS feed\")\n",
        "                print(f\"Fetching full content (filtering for articles >150 words)..\\n\")\n",
        "\n",
        "            # Process articles until we have max_articles that meet the word count requirement\n",
        "            articles_processed = 0\n",
        "            articles_skipped = 0\n",
        "\n",
        "            for entry in feed.entries:\n",
        "                # Stop if we have enough articles\n",
        "                if len(articles) >= max_articles:\n",
        "                    break\n",
        "\n",
        "                articles_processed += 1\n",
        "                article_url = entry.get('link', '').strip()\n",
        "                title = entry.get('title', '').strip()\n",
        "\n",
        "                if not article_url:\n",
        "                    continue\n",
        "\n",
        "                if verbose:\n",
        "                    title_short = title[:60] + \"...\" if len(title) > 60 else title\n",
        "                    print(f\"[{articles_processed}] Fetching: {title_short}...\")\n",
        "\n",
        "                # Extract full article text\n",
        "                full_text = self._extract_article_text(article_url)\n",
        "\n",
        "                # Parse publication date\n",
        "                published = entry.get('published', '')\n",
        "                published_datetime = None\n",
        "                if hasattr(entry, 'published_parsed') and entry.published_parsed:\n",
        "                    try:\n",
        "                        published_datetime = datetime(*entry.published_parsed[:6])\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                # Calculate word count\n",
        "                word_count = len(full_text.split()) if full_text else 0\n",
        "\n",
        "                # Filter: Only keep articles with more than 150 words\n",
        "                if word_count <= 150:\n",
        "                    articles_skipped += 1\n",
        "                    if verbose:\n",
        "                        print(f\"    âš  Skipped: {word_count} words (minimum 150 required)\")\n",
        "                    continue\n",
        "\n",
        "                article = {\n",
        "                    'ticker': ticker.upper(),\n",
        "                    'title': title,\n",
        "                    'link': article_url,\n",
        "                    'rss_description': entry.get('summary', '').strip(),\n",
        "                    'published': published,\n",
        "                    'published_datetime': published_datetime,\n",
        "                    'guid': entry.get('guid', ''),\n",
        "                    'full_text': full_text or '',\n",
        "                    'word_count': word_count,\n",
        "                    'has_full_text': full_text is not None and len(full_text) > 0\n",
        "                }\n",
        "\n",
        "                articles.append(article)\n",
        "\n",
        "                if verbose and full_text:\n",
        "                    print(f\"    âœ“ Retrieved {word_count} words\")\n",
        "                elif verbose:\n",
        "                    print(f\"    âš  Could not extract content\")\n",
        "\n",
        "            # Summary\n",
        "            if verbose:\n",
        "                print(f\"\\nðŸ“Š Summary:\")\n",
        "                print(f\"   Articles processed: {articles_processed}\")\n",
        "                print(f\"   Articles skipped (<150 words): {articles_skipped}\")\n",
        "                print(f\"   Articles returned: {len(articles)}\")\n",
        "\n",
        "            return articles\n",
        "\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"  âœ— Error: {e}\")\n",
        "            return articles\n",
        "\n",
        "# Initialize scraper\n",
        "scraper = YahooFinanceFullArticleScraper()\n",
        "\n",
        "# Get full articles for a ticker (e.g., 'AAPL')\n",
        "# The 'articles' variable will contain a list of dictionaries, each with full article content\n",
        "articles_for_nlp = scraper.get_full_articles_for_ticker('AAPL', max_articles=5, verbose=False)"
      ],
      "metadata": {
        "id": "xkWH3Szq73Du"
      },
      "id": "xkWH3Szq73Du",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_for_nlp[0]['full_text']"
      ],
      "metadata": {
        "id": "8BP8DJKu73fF"
      },
      "id": "8BP8DJKu73fF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "articles_for_nlp[4]['full_text']"
      ],
      "metadata": {
        "id": "gcj3Hda78WZO"
      },
      "id": "gcj3Hda78WZO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model load"
      ],
      "metadata": {
        "id": "CC0BcGHJyLPG"
      },
      "id": "CC0BcGHJyLPG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FinBERT"
      ],
      "metadata": {
        "id": "nZ2JWWpwyNJD"
      },
      "id": "nZ2JWWpwyNJD"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"ProsusAI/finbert\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "LmW0LzAp9hlU"
      },
      "id": "LmW0LzAp9hlU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis"
      ],
      "metadata": {
        "id": "-FmAIDcG03oJ"
      },
      "id": "-FmAIDcG03oJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AAPL"
      ],
      "metadata": {
        "id": "WddH9sA23Hdr"
      },
      "id": "WddH9sA23Hdr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract article text (summary or description, fallback to title)\n",
        "article_text = articles_for_nlp[0]['full_text']\n",
        "\n",
        "# Tokenize the extracted text\n",
        "inputs = tokenizer(article_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass tokenized input through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "# Get the predicted sentiment\n",
        "predicted_class_id = probabilities.argmax().item()\n",
        "sentiment = model.config.id2label[predicted_class_id]\n",
        "\n",
        "print(f\"Article Text: {article_text}\")\n",
        "print(f\"Sentiment Probabilities: {probabilities}\")\n",
        "print(f\"Predicted Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "jtUD1JoLyQGJ"
      },
      "id": "jtUD1JoLyQGJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract article text (summary or description, fallback to title)\n",
        "article_text = articles_for_nlp[1]['full_text']\n",
        "\n",
        "# Tokenize the extracted text\n",
        "inputs = tokenizer(article_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass tokenized input through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "# Get the predicted sentiment\n",
        "predicted_class_id = probabilities.argmax().item()\n",
        "sentiment = model.config.id2label[predicted_class_id]\n",
        "\n",
        "print(f\"Article Text: {article_text}\")\n",
        "print(f\"Sentiment Probabilities: {probabilities}\")\n",
        "print(f\"Predicted Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "T-Qp-GZ80kzo"
      },
      "id": "T-Qp-GZ80kzo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract article text (summary or description, fallback to title)\n",
        "article_text = articles_for_nlp[2]['full_text']\n",
        "\n",
        "# Tokenize the extracted text\n",
        "inputs = tokenizer(article_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass tokenized input through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "# Get the predicted sentiment\n",
        "predicted_class_id = probabilities.argmax().item()\n",
        "sentiment = model.config.id2label[predicted_class_id]\n",
        "\n",
        "print(f\"Article Text: {article_text}\")\n",
        "print(f\"Sentiment Probabilities: {probabilities}\")\n",
        "print(f\"Predicted Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "Rc4XhTiC0pqg"
      },
      "id": "Rc4XhTiC0pqg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract article text (summary or description, fallback to title)\n",
        "article_text = articles_for_nlp[3]['full_text']\n",
        "\n",
        "# Tokenize the extracted text\n",
        "inputs = tokenizer(article_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass tokenized input through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "# Get the predicted sentiment\n",
        "predicted_class_id = probabilities.argmax().item()\n",
        "sentiment = model.config.id2label[predicted_class_id]\n",
        "\n",
        "print(f\"Article Text: {article_text}\")\n",
        "print(f\"Sentiment Probabilities: {probabilities}\")\n",
        "print(f\"Predicted Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "icwCIgqP0sIK"
      },
      "id": "icwCIgqP0sIK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract article text (summary or description, fallback to title)\n",
        "article_text = articles_for_nlp[4]['full_text']\n",
        "\n",
        "# Tokenize the extracted text\n",
        "inputs = tokenizer(article_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass tokenized input through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "# Get the predicted sentiment\n",
        "predicted_class_id = probabilities.argmax().item()\n",
        "sentiment = model.config.id2label[predicted_class_id]\n",
        "\n",
        "print(f\"Article Text: {article_text}\")\n",
        "print(f\"Sentiment Probabilities: {probabilities}\")\n",
        "print(f\"Predicted Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "9umA4-nP0uHL"
      },
      "id": "9umA4-nP0uHL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bitcoin (BTC-USD)"
      ],
      "metadata": {
        "id": "aQHuNeeQ3MoR"
      },
      "id": "aQHuNeeQ3MoR"
    },
    {
      "cell_type": "code",
      "source": [
        "articles_for_nlp = scraper.get_full_articles_for_ticker('BTC-USD', max_articles=5, verbose=False)"
      ],
      "metadata": {
        "id": "VVtjjcM60yi5"
      },
      "id": "VVtjjcM60yi5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract article text (summary or description, fallback to title)\n",
        "article_text = articles_for_nlp[0]['full_text']\n",
        "\n",
        "# Tokenize the extracted text\n",
        "inputs = tokenizer(article_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass tokenized input through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "# Get the predicted sentiment\n",
        "predicted_class_id = probabilities.argmax().item()\n",
        "sentiment = model.config.id2label[predicted_class_id]\n",
        "\n",
        "print(f\"Article Text: {article_text}\")\n",
        "print(f\"Sentiment Probabilities: {probabilities}\")\n",
        "print(f\"Predicted Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "LUHnUlVY3ZxW"
      },
      "id": "LUHnUlVY3ZxW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract article text (summary or description, fallback to title)\n",
        "article_text = articles_for_nlp[1]['full_text']\n",
        "\n",
        "# Tokenize the extracted text\n",
        "inputs = tokenizer(article_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass tokenized input through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "# Get the predicted sentiment\n",
        "predicted_class_id = probabilities.argmax().item()\n",
        "sentiment = model.config.id2label[predicted_class_id]\n",
        "\n",
        "print(f\"Article Text: {article_text}\")\n",
        "print(f\"Sentiment Probabilities: {probabilities}\")\n",
        "print(f\"Predicted Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "owFA0Obt3sTc"
      },
      "id": "owFA0Obt3sTc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract article text (summary or description, fallback to title)\n",
        "article_text = articles_for_nlp[2]['full_text']\n",
        "\n",
        "# Tokenize the extracted text\n",
        "inputs = tokenizer(article_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass tokenized input through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "# Get the predicted sentiment\n",
        "predicted_class_id = probabilities.argmax().item()\n",
        "sentiment = model.config.id2label[predicted_class_id]\n",
        "\n",
        "print(f\"Article Text: {article_text}\")\n",
        "print(f\"Sentiment Probabilities: {probabilities}\")\n",
        "print(f\"Predicted Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "OI5KnwMy3uzs"
      },
      "id": "OI5KnwMy3uzs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract article text (summary or description, fallback to title)\n",
        "article_text = articles_for_nlp[3]['full_text']\n",
        "\n",
        "# Tokenize the extracted text\n",
        "inputs = tokenizer(article_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass tokenized input through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "# Get the predicted sentiment\n",
        "predicted_class_id = probabilities.argmax().item()\n",
        "sentiment = model.config.id2label[predicted_class_id]\n",
        "\n",
        "print(f\"Article Text: {article_text}\")\n",
        "print(f\"Sentiment Probabilities: {probabilities}\")\n",
        "print(f\"Predicted Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "npUaSgxY3x-j"
      },
      "id": "npUaSgxY3x-j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract article text (summary or description, fallback to title)\n",
        "article_text = articles_for_nlp[4]['full_text']\n",
        "\n",
        "# Tokenize the extracted text\n",
        "inputs = tokenizer(article_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass tokenized input through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "\n",
        "# Get the predicted sentiment\n",
        "predicted_class_id = probabilities.argmax().item()\n",
        "sentiment = model.config.id2label[predicted_class_id]\n",
        "\n",
        "print(f\"Article Text: {article_text}\")\n",
        "print(f\"Sentiment Probabilities: {probabilities}\")\n",
        "print(f\"Predicted Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "D6oZvo3A5UFX"
      },
      "id": "D6oZvo3A5UFX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA"
      ],
      "metadata": {
        "id": "4mbbHbQW5rLk"
      },
      "id": "4mbbHbQW5rLk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apple (AAPL)"
      ],
      "metadata": {
        "id": "NMxCRnYI8vy_"
      },
      "id": "NMxCRnYI8vy_"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "class StockArticleLDA:\n",
        "    \"\"\"\n",
        "    LDA Topic Extraction for Stock News Articles\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_topics=3, max_iter=50, random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize LDA analyzer\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_topics : int, default=3\n",
        "            Number of topics to extract (recommended 2-4 for 5 articles)\n",
        "        max_iter : int, default=50\n",
        "            Maximum iterations for LDA\n",
        "        random_state : int, default=42\n",
        "            Random state for reproducibility\n",
        "        \"\"\"\n",
        "        self.n_topics = n_topics\n",
        "        self.max_iter = max_iter\n",
        "        self.random_state = random_state\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        # Enhanced stopwords (general + finance-specific)\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        finance_stopwords = {\n",
        "            'said', 'say', 'says', 'company', 'companies', 'stock', 'stocks',\n",
        "            'share', 'shares', 'market', 'markets', 'new', 'also', 'would',\n",
        "            'could', 'may', 'might', 'one', 'two', 'first', 'last', 'year',\n",
        "            'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'\n",
        "        }\n",
        "        self.stop_words.update(finance_stopwords)\n",
        "\n",
        "        self.vectorizer = None\n",
        "        self.lda_model = None\n",
        "        self.feature_names = None\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Clean and preprocess text\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        text : str\n",
        "            Raw article text\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        str : Cleaned and lemmatized text\n",
        "        \"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remove email addresses\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "        # Remove special characters and digits (keeping spaces)\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords, short words, and lemmatize\n",
        "        cleaned_tokens = []\n",
        "        for token in tokens:\n",
        "            if (len(token) >= 3 and\n",
        "                token not in self.stop_words and\n",
        "                token not in string.punctuation):\n",
        "                lemmatized = self.lemmatizer.lemmatize(token, pos='v')  # Verb lemmatization\n",
        "                lemmatized = self.lemmatizer.lemmatize(lemmatized, pos='n')  # Noun lemmatization\n",
        "                cleaned_tokens.append(lemmatized)\n",
        "\n",
        "        return ' '.join(cleaned_tokens)\n",
        "\n",
        "    def extract_articles(self, articles_dict, n_articles=5):\n",
        "        \"\"\"\n",
        "        Extract and preprocess articles from dictionary\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        articles_dict : dict\n",
        "            Dictionary containing articles with 'full_text' key\n",
        "        n_articles : int, default=5\n",
        "            Number of articles to extract\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        list : List of preprocessed article texts\n",
        "        \"\"\"\n",
        "        preprocessed_articles = []\n",
        "\n",
        "        for i in range(n_articles):\n",
        "            try:\n",
        "                raw_text = articles_dict[i]['full_text']\n",
        "                cleaned_text = self.preprocess_text(raw_text)\n",
        "                preprocessed_articles.append(cleaned_text)\n",
        "                print(f\"âœ“ Article {i} preprocessed ({len(cleaned_text)} characters)\")\n",
        "            except (KeyError, IndexError) as e:\n",
        "                print(f\"âš  Warning: Could not process article {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return preprocessed_articles\n",
        "\n",
        "    def fit_lda(self, articles, use_tfidf=True, max_features=1000, min_df=1, max_df=0.95):\n",
        "        \"\"\"\n",
        "        Fit LDA model on preprocessed articles\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        articles : list\n",
        "            List of preprocessed article texts\n",
        "        use_tfidf : bool, default=True\n",
        "            Use TF-IDF vectorization (recommended for news articles)\n",
        "        max_features : int, default=1000\n",
        "            Maximum number of features for vectorization\n",
        "        min_df : int/float, default=1\n",
        "            Minimum document frequency\n",
        "        max_df : float, default=0.95\n",
        "            Maximum document frequency (remove very common terms)\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        self : Returns instance for method chaining\n",
        "        \"\"\"\n",
        "        if len(articles) < 2:\n",
        "            raise ValueError(\"Need at least 2 articles for LDA. Found: {}\".format(len(articles)))\n",
        "\n",
        "        print(f\"\\nðŸ“Š Vectorizing {len(articles)} articles...\")\n",
        "\n",
        "        # Vectorization with TF-IDF or Count\n",
        "        if use_tfidf:\n",
        "            self.vectorizer = TfidfVectorizer(\n",
        "                max_features=max_features,\n",
        "                min_df=min_df,\n",
        "                max_df=max_df,\n",
        "                ngram_range=(1, 2)  # Include bigrams for better topic coherence\n",
        "            )\n",
        "        else:\n",
        "            self.vectorizer = CountVectorizer(\n",
        "                max_features=max_features,\n",
        "                min_df=min_df,\n",
        "                max_df=max_df,\n",
        "                ngram_range=(1, 2)\n",
        "            )\n",
        "\n",
        "        # Transform articles to document-term matrix\n",
        "        doc_term_matrix = self.vectorizer.fit_transform(articles)\n",
        "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
        "\n",
        "        print(f\"âœ“ Document-term matrix shape: {doc_term_matrix.shape}\")\n",
        "        print(f\"âœ“ Vocabulary size: {len(self.feature_names)}\")\n",
        "\n",
        "        # Fit LDA model\n",
        "        print(f\"\\nðŸ” Fitting LDA with {self.n_topics} topics...\")\n",
        "        self.lda_model = LatentDirichletAllocation(\n",
        "            n_components=self.n_topics,\n",
        "            max_iter=self.max_iter,\n",
        "            random_state=self.random_state,\n",
        "            learning_method='batch',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        self.lda_model.fit(doc_term_matrix)\n",
        "\n",
        "        print(f\"âœ“ LDA model fitted successfully\")\n",
        "        print(f\"âœ“ Log-likelihood: {self.lda_model.score(doc_term_matrix):.2f}\")\n",
        "        print(f\"âœ“ Perplexity: {self.lda_model.perplexity(doc_term_matrix):.2f}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_top_words_per_topic(self, n_words=10):\n",
        "        \"\"\"\n",
        "        Extract top words for each topic\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_words : int, default=10\n",
        "            Number of top words to extract per topic\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Dictionary with topic numbers as keys and lists of top words as values\n",
        "        \"\"\"\n",
        "        if self.lda_model is None:\n",
        "            raise ValueError(\"Model not fitted. Call fit_lda() first.\")\n",
        "\n",
        "        topics_dict = {}\n",
        "\n",
        "        for topic_idx, topic in enumerate(self.lda_model.components_):\n",
        "            top_indices = topic.argsort()[-n_words:][::-1]\n",
        "            top_words = [self.feature_names[i] for i in top_indices]\n",
        "            top_weights = [topic[i] for i in top_indices]\n",
        "\n",
        "            topics_dict[f\"Topic {topic_idx + 1}\"] = {\n",
        "                'words': top_words,\n",
        "                'weights': top_weights\n",
        "            }\n",
        "\n",
        "        return topics_dict\n",
        "\n",
        "    def get_document_topic_distribution(self, articles):\n",
        "        \"\"\"\n",
        "        Get topic distribution for each article\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        articles : list\n",
        "            List of preprocessed article texts\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        numpy.ndarray : Array of shape (n_articles, n_topics) with topic probabilities\n",
        "        \"\"\"\n",
        "        if self.lda_model is None or self.vectorizer is None:\n",
        "            raise ValueError(\"Model not fitted. Call fit_lda() first.\")\n",
        "\n",
        "        doc_term_matrix = self.vectorizer.transform(articles)\n",
        "        topic_distributions = self.lda_model.transform(doc_term_matrix)\n",
        "\n",
        "        return topic_distributions\n",
        "\n",
        "    def display_results(self, articles, n_words=10):\n",
        "        \"\"\"\n",
        "        Display comprehensive LDA results\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        articles : list\n",
        "            List of preprocessed article texts\n",
        "        n_words : int, default=10\n",
        "            Number of top words to display per topic\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ðŸ“° LDA TOPIC EXTRACTION RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Display topics\n",
        "        topics = self.get_top_words_per_topic(n_words)\n",
        "\n",
        "        for topic_name, topic_data in topics.items():\n",
        "            print(f\"\\n{topic_name}:\")\n",
        "            print(\"-\" * 40)\n",
        "            for word, weight in zip(topic_data['words'], topic_data['weights']):\n",
        "                print(f\"  {word:20s} {weight:8.4f}\")\n",
        "\n",
        "        # Display document-topic distribution\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ðŸ“„ ARTICLE-TOPIC DISTRIBUTIONS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        topic_dist = self.get_document_topic_distribution(articles)\n",
        "\n",
        "        for i, dist in enumerate(topic_dist):\n",
        "            print(f\"\\nArticle {i}:\")\n",
        "            for topic_idx, prob in enumerate(dist):\n",
        "                print(f\"  Topic {topic_idx + 1}: {prob:6.2%}\")\n",
        "            dominant_topic = np.argmax(dist) + 1\n",
        "            print(f\"  â†’ Dominant Topic: Topic {dominant_topic}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# USAGE EXAMPLE\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_stock_articles(articles_for_nlp, n_topics=3):\n",
        "    \"\"\"\n",
        "    Main function to analyze stock articles using LDA\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    articles_for_nlp : dict\n",
        "        Dictionary containing articles with structure articles_for_nlp[i]['full_text']\n",
        "    n_topics : int, default=3\n",
        "        Number of topics to extract\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    StockArticleLDA : Fitted LDA analyzer object\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = StockArticleLDA(n_topics=n_topics)\n",
        "\n",
        "    # Extract and preprocess articles\n",
        "    print(\"ðŸ”„ Extracting and preprocessing articles...\")\n",
        "    print(\"-\" * 80)\n",
        "    preprocessed_articles = analyzer.extract_articles(articles_for_nlp, n_articles=5)\n",
        "\n",
        "    if len(preprocessed_articles) < 2:\n",
        "        print(\"âŒ Error: Need at least 2 valid articles for LDA analysis\")\n",
        "        return None\n",
        "\n",
        "    # Fit LDA model\n",
        "    analyzer.fit_lda(preprocessed_articles, use_tfidf=True)\n",
        "\n",
        "    # Display results\n",
        "    analyzer.display_results(preprocessed_articles, n_words=10)\n",
        "\n",
        "    return analyzer\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# QUICK START - Uncomment to run\n",
        "# =============================================================================\n",
        "\n",
        "# Example usage (uncomment when ready to run):\n",
        "# analyzer = analyze_stock_articles(articles_for_nlp, n_topics=3)\n",
        "\n",
        "# To get topic keywords programmatically:\n",
        "# topics = analyzer.get_top_words_per_topic(n_words=10)\n",
        "\n",
        "# To experiment with different number of topics:\n",
        "# for k in range(2, 5):\n",
        "#     print(f\"\\n{'='*80}\\nTesting with {k} topics\\n{'='*80}\")\n",
        "#     analyzer = analyze_stock_articles(articles_for_nlp, n_topics=k)"
      ],
      "metadata": {
        "id": "D2ILqFHE-7jQ"
      },
      "id": "D2ILqFHE-7jQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer = analyze_stock_articles(articles_for_nlp, n_topics=3)\n",
        "\n",
        "# To get topic keywords programmatically:\n",
        "# topics = analyzer.get_top_words_per_topic(n_words=10)\n",
        "\n",
        "# To experiment with different number of topics:\n",
        "# for k in range(2, 5):\n",
        "#     print(f\"\\n{'='*80}\\nTesting with {k} topics\\n{'='*80}\")\n",
        "#     analyzer = analyze_stock_articles(articles_for_nlp, n_topics=k)"
      ],
      "metadata": {
        "id": "qrwx3f905omo"
      },
      "id": "qrwx3f905omo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}